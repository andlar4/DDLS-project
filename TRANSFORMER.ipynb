{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define amino acid and secondary structure mappings\n",
    "amino_acid_vocab = {'A': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, \n",
    "                    'K': 9, 'L': 10, 'M': 11, 'N': 12, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, \n",
    "                    'T': 17, 'V': 18, 'W': 19, 'Y': 20, 'X': 0}  # X for padding\n",
    "sst3_vocab = {'C': 0, 'E': 1, 'H': 2}  # Coil, Beta-sheet, Alpha-helix\n",
    "\n",
    "\n",
    "#Function to one hot encode a sequence\n",
    "\n",
    "def one_hot_encode(sequence):\n",
    "\n",
    "    one_hot = np.zeros((len(sequence), len(amino_acid_vocab)), dtype=int)\n",
    "    for idx, aa in enumerate(sequence):\n",
    "        one_hot[idx, amino_acid_vocab.get(aa, 0)] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "#Function to extract one hot encoded sequences, and remove those close to the edge.\n",
    "\n",
    "def extract_features_and_labels(sequences, structures, window_size=15):\n",
    "\n",
    "    X, y = [], []\n",
    "    half_window = window_size // 2\n",
    "    \n",
    "    for sequence, structure in zip(sequences, structures):\n",
    "        padded_sequence = 'X' * half_window + sequence + 'X' * half_window  # Padding with 'X'\n",
    "        for i in range(len(sequence)):\n",
    "            window = padded_sequence[i:i + window_size]  # Extract window\n",
    "            if 'X' not in window:  # Ignore windows with padding (if any)\n",
    "                one_hot_window = one_hot_encode(window)  # One-hot encode the window\n",
    "                X.append(one_hot_window.flatten())  # Flatten to 1D\n",
    "                y.append(structure[i])  # Label for the center amino acid\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class to load sequences and labels\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, data_file, window_size=15):\n",
    "        self.data = pd.read_csv(data_file)\n",
    "        self.window_size = window_size\n",
    "        self.X, self.y = self.prepare_data()\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        sequences = self.data['seq'].tolist()\n",
    "        structures = self.data['sst3'].tolist()\n",
    "        X, y = extract_features_and_labels(sequences, structures, self.window_size)\n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Convert to tensors\n",
    "        input_seq = torch.tensor(self.X[idx], dtype=torch.float32)\n",
    "        target_sst3 = torch.tensor(sst3_vocab[self.y[idx]], dtype=torch.long)\n",
    "        \n",
    "        return input_seq, target_sst3\n",
    "\n",
    "# Model definition\n",
    "class ProteinSecondaryStructureModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, n_heads, num_encoder_layers, hidden_dim, output_dim, dropout=0):\n",
    "        super(ProteinSecondaryStructureModel, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, embedding_dim)  # Input layer to embedding\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=n_heads, dim_feedforward=hidden_dim, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.fc2 = nn.Linear(embedding_dim, output_dim)  # Output layer\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.fc1(src)  # Convert input to embedding dimension\n",
    "        seq_len = src.size(0)  # Assuming input is (seq_len, batch_size, input_dim)\n",
    "        src = src.unsqueeze(1)  # Add batch dimension\n",
    "        transformer_output = self.transformer_encoder(src)\n",
    "        output = self.fc2(transformer_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function for training the model\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs, output_dim):\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        # Training loop\n",
    "        for i, (input_seq, target_sst3) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(input_seq)\n",
    "\n",
    "            # Reshape for loss computation\n",
    "            output = output.view(-1, output_dim)  # (batch_size*seq_len, output_dim)\n",
    "            target_sst3 = target_sst3.view(-1)  # (batch_size*seq_len)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(output, target_sst3)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Compute training accuracy\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct_train += (predicted == target_sst3).sum().item()\n",
    "            total_train += target_sst3.size(0)\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "        \n",
    "        # Store training loss and accuracy\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # Evaluate on test set after each epoch\n",
    "        test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, output_dim)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
    "        print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "    return train_losses, train_accuracies, test_accuracies\n",
    "\n",
    "# Function for evaluating the model\n",
    "def evaluate_model(model, test_loader, criterion, output_dim):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_sst3 in test_loader:\n",
    "            output = model(input_seq)\n",
    "            output = output.view(-1, output_dim)\n",
    "            target_sst3 = target_sst3.view(-1)\n",
    "\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct += (predicted == target_sst3).sum().item()\n",
    "            total += target_sst3.size(0)\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return None, accuracy  # Returning `None` for loss since we don't track test loss\n",
    "\n",
    "# Function to plot training and test accuracy\n",
    "def plot_metrics(train_losses, train_accuracies, test_accuracies):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label='Training Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss over Epochs')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(epochs, test_accuracies, label='Test Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize and run the model\n",
    "def run_experiment(train_file, test_file, num_epochs=10, batch_size=32, input_dim=315, embedding_dim=128, n_heads=8, num_encoder_layers=4, hidden_dim=256, dropout=0.1):\n",
    "    output_dim = len(sst3_vocab)\n",
    "    \n",
    "    # Initialize dataset and dataloader\n",
    "    train_dataset = ProteinDataset(train_file, window_size=15)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    test_dataset = ProteinDataset(test_file, window_size=15)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = ProteinSecondaryStructureModel(input_dim=input_dim, embedding_dim=embedding_dim, n_heads=n_heads, num_encoder_layers=num_encoder_layers, hidden_dim=hidden_dim, output_dim=output_dim, dropout=dropout)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    train_losses, train_accuracies, test_accuracies = train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs, output_dim)\n",
    "    \n",
    "    plot_metrics(train_losses, train_accuracies, test_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the run_experiment function\n",
    "train_file = 'training_data.csv'\n",
    "test_file = 'test_data.csv'\n",
    "    \n",
    "# Set hyperparameters\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "input_dim = 15 * len(amino_acid_vocab)\n",
    "embedding_dim = 128\n",
    "n_heads = 8\n",
    "num_encoder_layers = 4\n",
    "hidden_dim = 256\n",
    "dropout = 0\n",
    "    \n",
    "run_experiment(\n",
    "    train_file, test_file, \n",
    "    num_epochs=num_epochs, \n",
    "    batch_size=batch_size, \n",
    "    input_dim=input_dim,\n",
    "    embedding_dim=embedding_dim, \n",
    "    n_heads=n_heads, \n",
    "    num_encoder_layers=num_encoder_layers, \n",
    "    hidden_dim=hidden_dim, \n",
    "    dropout=dropout\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
