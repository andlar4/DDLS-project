{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    sequences = data['seq'].tolist()\n",
    "    structures = data['sst3'].tolist()\n",
    "    \n",
    "    return sequences, structures\n",
    "\n",
    "AMINO_ACIDS = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "\n",
    "#Function to one hot encode a sequence\n",
    "def one_hot_encode(sequence):\n",
    "\n",
    "    one_hot_matrix = np.zeros((len(sequence), len(AMINO_ACIDS)), dtype=int)\n",
    "    \n",
    "    for i, amino_acid in enumerate(sequence):\n",
    "        if amino_acid in AMINO_ACIDS:\n",
    "            index = AMINO_ACIDS.index(amino_acid)\n",
    "            one_hot_matrix[i, index] = 1\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown amino acid {amino_acid} in sequence.\")\n",
    "    \n",
    "    return one_hot_matrix\n",
    "\n",
    "\n",
    "#Function to extract one hot encoded sequences, and remove those close to the edge.\n",
    "def extract_features_and_labels(sequences, structures, window_size=15):\n",
    "\n",
    "    X, y = [], []\n",
    "    half_window = window_size // 2\n",
    "    \n",
    "    for sequence, structure in zip(sequences, structures):\n",
    "        padded_sequence = 'X' * half_window + sequence + 'X' * half_window  # Padding with 'X'\n",
    "        for i in range(len(sequence)):\n",
    "            window = padded_sequence[i:i + window_size]  # Extract window\n",
    "            if 'X' not in window:  # Ignore windows with padding (if any)\n",
    "                one_hot_window = one_hot_encode(window)  # One-hot encode the window\n",
    "                X.append(one_hot_window.flatten())  # Flatten to 1D\n",
    "                y.append(structure[i])  # Label for the center amino acid\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "# Convert structures to integers for classification (C = 0, E = 1, H = 2)\n",
    "def preprocess_labels(y):\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y) \n",
    "    return y_encoded, label_encoder\n",
    "\n",
    "def prepare_data(X, y, batch_size=64):\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32) \n",
    "    y_tensor = torch.tensor(y, dtype=torch.long)     \n",
    "    \n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute accuracy\n",
    "def accuracy(preds, labels):\n",
    "    _, predicted = torch.max(preds, 1)\n",
    "    return (predicted == labels).sum().item() / len(labels)\n",
    "\n",
    "# Training function\n",
    "\n",
    "def train_rnn(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device='cpu'):\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    train_accuracies = [] \n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0  \n",
    "        total_train = 0  \n",
    "\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        \n",
    "            inputs = inputs.view(-1, 15, 20)  # (batch_size, seq_len, input_size)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct_train / total_train\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        # Evaluation on validation set\n",
    "        val_acc = evaluate_rnn(model, val_loader, device)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n",
    "\n",
    "    return train_losses, train_accuracies, val_accuracies\n",
    "\n",
    "# Validation function\n",
    "def evaluate_rnn(model, val_loader, device='cpu'):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Reshape inputs to 3D (batch_size, sequence_length, input_size)\n",
    "            inputs = inputs.view(-1, 15, 20)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Define an RNN layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)  # batch_first=True makes input (batch, seq, input_size)\n",
    "        \n",
    "        # Fully connected layer for output prediction\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state (batch_size, hidden_size)\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)  # Initialize hidden state\n",
    "        \n",
    "        # Forward propagate through RNN\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        \n",
    "        # We take the output of the last time step\n",
    "        out = out[:, -1, :]  # (batch_size, hidden_size)\n",
    "        \n",
    "        # Pass through the fully connected layer\n",
    "        out = self.fc(out) \n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2, dropout = 0):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Define a two-layer LSTM\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)  # batch_first=True makes input (batch, seq, input_size)\n",
    "        \n",
    "        # Dropout layer before the fully connected layer (optional, for further regularization)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Fully connected layer for output prediction\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Initialize hidden state and cell state for LSTM\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)  # Hidden state\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)  # Cell state\n",
    "        \n",
    "        # Forward propagate through LSTM\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # We take the output of the last time step\n",
    "        out = out[:, -1, :]  # (batch_size, hidden_size)\n",
    "\n",
    "        # Apply dropout before the fully connected layer\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Pass through the fully connected layer to map hidden state to output\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Define a two-layer GRU\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)  # Use GRU instead of LSTM\n",
    "        \n",
    "        # Fully connected layer for output prediction\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Initialize hidden state for GRU\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)  # GRU only has hidden state, no cell state\n",
    "\n",
    "        # Forward propagate through GRU\n",
    "        out, hn = self.gru(x, h0)\n",
    "        \n",
    "        # We take the output of the last time step\n",
    "        out = out[:, -1, :]  # (batch_size, hidden_size)\n",
    "        \n",
    "        # Pass through the fully connected layer to map hidden state to output\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training and test data\n",
    "train_file = 'training_data.csv'\n",
    "test_file = 'test_data.csv'\n",
    "\n",
    "train_sequences, train_structures = load_data(train_file)\n",
    "test_sequences, test_structures = load_data(test_file)\n",
    "\n",
    "# Feature extraction (sliding window and one-hot encoding)\n",
    "X_train, y_train = extract_features_and_labels(train_sequences, train_structures)\n",
    "X_test, y_test = extract_features_and_labels(test_sequences, test_structures)\n",
    "\n",
    "# Preprocess labels (encode them)\n",
    "y_train_encoded, label_encoder = preprocess_labels(y_train)\n",
    "y_test_encoded, _ = preprocess_labels(y_test)  # Use the same label encoder for test set\n",
    "\n",
    "# Create DataLoaders for training and testing\n",
    "train_loader = prepare_data(X_train, y_train_encoded)\n",
    "test_loader = prepare_data(X_test, y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 20  # One-hot encoded amino acid (20 possible values for each amino acid)\n",
    "hidden_size = 64\n",
    "output_size = 3  # C, E, H (3 secondary structure types)\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#Choose the desired model, loss function and optimiser\n",
    "model = VanillaRNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Train the model \n",
    "train_losses, train_accuracies, val_accuracies = train_rnn(model, train_loader, test_loader, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "# Plot the training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the training accuracy and validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_accuracies, label='Training Accuracy', color='blue')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy', color='orange')\n",
    "plt.title('Training and Validation Accuracy Over Time')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
